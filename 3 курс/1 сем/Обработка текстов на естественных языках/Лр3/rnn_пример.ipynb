{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "C1IMoqFPZ-_E"
      },
      "cell_type": "markdown",
      "source": [
        "<!-- #  Семинар: Рекурентные нейронные сети. -->\n",
        "\n",
        "В данной работе вам предлагается посмотреть на всю мощь рекурентных нейронных сетей решив небольшую задачу.\n",
        "\n",
        " Предлагаю решить вам задачу расшифровки сообщения с помощью RNN.\n",
        " Представьте, что вам даны сообщения зашифрованные с помощью шифра Цезаря, являющимся одним из самый простых шифров в криптографии.\n"
      ]
    },
    {
      "metadata": {
        "id": "1YNGZnxua61K"
      },
      "cell_type": "markdown",
      "source": [
        "Шифр цезаря работает следующим образом: каждя буква\n",
        "исходного алфавита сдвигается на K символов вправо:\n",
        "\n",
        "Пусть нам дано сообщение: message=\"RNN IS NOT AI\", тогда наше шифрование выполняющиеся по правилу f, с K=2, даст нам результат:\n",
        "f(message, K) = TPPAKUAPQVACK\n",
        "\n",
        "Для удобство можно взять символы только одного регистра в нашей имплементации, и сказать, что все буквы не английского алфавита будут отмечены как прочерк \"-\"."
      ]
    },
    {
      "metadata": {
        "id": "zPH9sAiDe77A"
      },
      "cell_type": "code",
      "source": [
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xoZVmCuEdO7Y"
      },
      "cell_type": "code",
      "source": [
        "# Определим ключ и словарь\n",
        "key = 2\n",
        "vocab = [char for char in ' -ABCDEFGHIJKLMNOPQRSTUVWXYZ']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yAsJDtSsGgOV",
        "outputId": "61365adb-eccf-4fe5-cf8c-43e5d65f89d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "# Напишем функцию, которая делает\n",
        "def encrypt(text, key):\n",
        "    \"\"\"Returns the encrypted form of 'text'.\"\"\"\n",
        "    indexes = [vocab.index(char) for char in text]\n",
        "    encrypted_indexes = [(idx + key) % len(vocab) for idx in indexes]\n",
        "    encrypted_chars = [vocab[idx] for idx in encrypted_indexes]\n",
        "    encrypted = ''.join(encrypted_chars)\n",
        "    return encrypted\n",
        "\n",
        "print(encrypt('RNN IS NOT AI', key))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TPPAKUAPQVACK\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "Z6QNnDJPdxXC"
      },
      "cell_type": "markdown",
      "source": [
        "Теперь нам необходимо нагенерировать датасет для решения задачи обучения с учителем. Нашим датасетом может быть случайно зашифрованные фразы, и тогда его структура будет следующей:\n",
        "message --- encrypted message\n",
        "\n",
        "Это пример параллельного корпуса из НЛП.\n",
        "\n",
        "Но нам необходимо представить каждую букву в виде ее номера в словаре, чтобы далее воспользоваться Embedding слоем.\n",
        "\n",
        "Для простоты давайте допустим, что все строки имеют одинаковую длину seq_len"
      ]
    },
    {
      "metadata": {
        "id": "Te9ugR1AIjEw"
      },
      "cell_type": "code",
      "source": [
        "num_examples = 256 # размер датасета\n",
        "seq_len = 18 # максимальная длина строки\n",
        "\n",
        "\n",
        "def encrypted_dataset(dataset_len, k):\n",
        "    \"\"\"\n",
        "    Return: List(Tuple(Tensor encrypted, Tensor source))\n",
        "    \"\"\"\n",
        "    dataset = []\n",
        "    for x in range(dataset_len):\n",
        "        random_message  = ''.join([random.choice(vocab) for x in range(seq_len)])\n",
        "        encrypt_random_message = encrypt(''.join(random_message), k)\n",
        "        src = [vocab.index(x) for x in random_message]\n",
        "        tgt = [vocab.index(x) for x in encrypt_random_message]\n",
        "        dataset.append([torch.tensor(tgt), torch.tensor(src)])\n",
        "    return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0RUxQqTCGUtx"
      },
      "cell_type": "markdown",
      "source": [
        "**Pytorch RNN:**\n",
        "$$h_t = \\text{tanh}(w_{ih} x_t + b_{ih} + w_{hh} h_{(t-1)} + b_{hh})$$\n",
        "\n",
        "**where : $h_t$ is the hidden state at time $t$, $x_t$ is\n",
        "    the input at time $t$, and $h_{(t-1)}$ is the hidden state of the\n",
        "    previous layer at time $t-1$ or the initial hidden state at time $0$.**\n",
        "    \n",
        "Args:\n",
        "\n",
        "        input_size: The number of expected features in the input $x$\n",
        "        hidden_size: The number of features in the hidden state $h$\n",
        "        num_layers: Number of recurrent layers. E.g., setting"
      ]
    },
    {
      "metadata": {
        "id": "03y7VB9QLorQ"
      },
      "cell_type": "code",
      "source": [
        "class Decipher(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim,\n",
        "                 rnn_type='simple'):\n",
        "        \"\"\"\n",
        "        :params: int vocab_size\n",
        "        :params: int embedding_dim\n",
        "        :params\n",
        "        \"\"\"\n",
        "        super(Decipher, self).__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embedding_dim)\n",
        "        if rnn_type == 'simple':\n",
        "            self.rnn = nn.RNN(embedding_dim, hidden_dim, num_layers = 2)\n",
        "\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "        self.initial_hidden = torch.zeros(2, 1, hidden_dim)\n",
        "\n",
        "\n",
        "    def forward(self, cipher):\n",
        "        # CHECK INPUT SIZE\n",
        "        # Unsqueeze 1 dimension for batches\n",
        "        embd_x = self.embed(cipher).unsqueeze(1)\n",
        "        out_rnn, hidden = self.rnn(embd_x, self.initial_hidden)\n",
        "        # Apply the affine transform and transpose output in appropriate way\n",
        "        # because you want to get the softmax on vocabulary dimension\n",
        "        # in order to get probability of every letter\n",
        "        return self.fc(out_rnn).transpose(1, 2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5rnb-2qIIjM7"
      },
      "cell_type": "code",
      "source": [
        "# определим параметры нашей модели\n",
        "embedding_dim = 5\n",
        "hidden_dim = 10\n",
        "vocab_size = len(vocab)\n",
        "lr = 1e-3\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Инициализируйте модель\n",
        "model = Decipher(vocab_size, embedding_dim, hidden_dim)\n",
        "\n",
        "# Инициализируйте оптимизатор: рекомендуется Adam\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
        "\n",
        "num_epochs = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yZO1uR7fIjQ9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "438a936b-ed0f-4490-8bd4-e0e8267391c9"
      },
      "cell_type": "code",
      "source": [
        "k = 10\n",
        "for x in range(num_epochs):\n",
        "    print('Epoch: {}'.format(x))\n",
        "    for encrypted, original in encrypted_dataset(num_examples, k):\n",
        "\n",
        "        scores = model(encrypted)\n",
        "        original = original.unsqueeze(1)\n",
        "        # Calculate loss\n",
        "        loss = criterion(scores, original)\n",
        "        # Zero grads\n",
        "        optimizer.zero_grad()\n",
        "        # Backpropagate\n",
        "        loss.backward()\n",
        "        # Update weights\n",
        "        optimizer.step()\n",
        "    print('Loss: {:6.4f}'.format(loss.item()))\n",
        "\n",
        "    with torch.no_grad():\n",
        "        matches, total = 0, 0\n",
        "        for encrypted, original in encrypted_dataset(num_examples, k):\n",
        "            # Compute a softmax over the outputs\n",
        "            predictions = F.softmax(model(encrypted), 1)\n",
        "            # Choose the character with the maximum probability (greedy decoding)\n",
        "            _, batch_out = predictions.max(dim=1)\n",
        "            # Remove batch\n",
        "            batch_out = batch_out.squeeze(1)\n",
        "            # Calculate accuracy\n",
        "            matches += torch.eq(batch_out, original).sum().item()\n",
        "            total += torch.numel(batch_out)\n",
        "        accuracy = matches / total\n",
        "        print('Accuracy: {:4.2f}%'.format(accuracy * 100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0\n",
            "Loss: 2.8407\n",
            "Accuracy: 32.49%\n",
            "Epoch: 1\n",
            "Loss: 1.8681\n",
            "Accuracy: 56.51%\n",
            "Epoch: 2\n",
            "Loss: 1.2646\n",
            "Accuracy: 76.80%\n",
            "Epoch: 3\n",
            "Loss: 1.0149\n",
            "Accuracy: 88.78%\n",
            "Epoch: 4\n",
            "Loss: 0.7237\n",
            "Accuracy: 91.47%\n",
            "Epoch: 5\n",
            "Loss: 0.4954\n",
            "Accuracy: 99.61%\n",
            "Epoch: 6\n",
            "Loss: 0.4877\n",
            "Accuracy: 99.39%\n",
            "Epoch: 7\n",
            "Loss: 0.3346\n",
            "Accuracy: 100.00%\n",
            "Epoch: 8\n",
            "Loss: 0.2074\n",
            "Accuracy: 100.00%\n",
            "Epoch: 9\n",
            "Loss: 0.1726\n",
            "Accuracy: 100.00%\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "krOVEf61IjUJ"
      },
      "cell_type": "code",
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "u9C5aMv1GUu9"
      },
      "cell_type": "code",
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-8E9YhMsGUvL"
      },
      "cell_type": "code",
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}